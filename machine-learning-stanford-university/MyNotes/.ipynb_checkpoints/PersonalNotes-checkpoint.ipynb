{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-229 Personal Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Hypotheses.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^{(i)}$ denotes I/P variables of $i'th$ training example.   \n",
    "$y^{(i)}$ denotes corresponding O/P  \n",
    "A pair $(x^{(i)}, y^{(i)})$ called a training example.  \n",
    "Set of $m$ training examples called training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here h is the function dependent on various features / parameters.\n",
    "Eg:-\n",
    "    Let cost of house be $y $, area be $x1$ and no. of room $x2$.  \n",
    "    $ h_{\\theta} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{1}x_{2} $  \n",
    "    $ \\theta_{i}$'s are called the parameters / weights.  \n",
    " ### $ h_{\\theta} = \\sum_{i=0}^{m}\\theta_{i}x_{i} = \\theta^{T}x$, where $x_{0} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta)$ denoted as the cost function.  \n",
    "### $J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMS Algorithm (Least mean square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose $\\theta$ such that to minimize $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with some initial guess for $\\theta$ and then repeatedly change $\\theta$ to minimize $J(\\theta)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat until convergence :\n",
    "### $\\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above rule called LMS update rule and also Widrow-Hoff learning rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here $\\alpha$ is called the learning rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we **simulataneosly update for all values of $j$.**  \n",
    "i.e  \n",
    "    $temp0 := \\theta_{0} - \\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}$  \n",
    "    $temp1 := \\theta_{1} - \\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_{1}}$  \n",
    "    $\\theta_{0} := temp0$  \n",
    "    $\\theta_{1} := temp1$      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} = \\frac{\\partial\\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^{2}}{\\partial \\theta_{j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  $ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}= \\frac{1}{2} \\cdot 2 \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot \\frac{\\partial (h_{\\theta}x^{(i)})}{\\partial \\theta_{j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}= \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat until convergence :\n",
    "### $\\theta_{j} := \\theta_{j} - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)} $   (for every j)\n",
    "### In vectorized form:\n",
    "$\\theta = \\theta - \\frac{\\alpha}{m} \\cdot X^{T}(X\\theta - y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This method looks at every traing example in the entire training set on every step, thus called batch gradient descent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/batchGradientDescent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above example we start at the above star point and then descend in the direction of steepest slope and then reach local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch gradient descent algo runs slow when training set is large, alternative algo below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent (incremental gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop {  \n",
    "&ensp;&ensp;&ensp;&ensp;for i = 1 to m {  \n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;$\\theta_{j} := \\theta_{j} - \\alpha \\cdot (h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)}$ (for every $j$)            \n",
    "&ensp;&ensp;&ensp;&ensp;}  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic is faster than batch algo, here parameters are updated after visiting a training example.\n",
    "However stichastic may never converge to a minimum, but will oscillate around the minimum .\n",
    "Stochastic preferred when training set is large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent feature scaling\n",
    "\n",
    "Gradient descent can be sped up by bringing each of input values in roughly the same range.\n",
    "This is becoz $/theta$ descends faster on small ranges and slower on larger ranges.\n",
    "For this we use feature scaling and mean normalization. Feature scaling corresponds to divind input by range i.e $(max - min)$. Mean normalization is subtracting input by mean of input over set.\n",
    "e.g :- \n",
    "\n",
    "$x_{i}$ ranges from $10$ to $30$ and its mean over the range is $23$ then $x_{i} = \\frac{x_{i}-23}{20}$\n",
    "\n",
    "thus,\n",
    "\n",
    "$x_{i} = \\frac{x_{i}-\\mu}{s_{i}}$\n",
    "\n",
    "where $\\mu$ is the mean, $s_{i}$ can be the standard deviation or $max-min$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging gradient descent: \n",
    "plot $J(\\theta)$ against number of iterations, if $J(\\theta)$ increases then $\\alpha$ is too largec\n",
    "\n",
    "if $\\alpha$ is too small, slow convergence, if $\\alpha$ is too large, cost function may overshoot and not converge.\n",
    "Try from $\\alpha = 0.001$ and then experiment with $3 * \\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\theta$ can also be found using the following equation:    \n",
    "### $\\theta = (X^{T}X)^{-1}X^{T}Y$    \n",
    "where $X$ is the m-by-n+1 design matrix containing all training example input values in it's rows,  $\\vec{y}^{\\,}$ is the m-dimensional vector.\n",
    "if m < n then $X^{T}X$ is non-invertible, if m = n then $X^{T}X$ may be non-invertible, thus we should moore-penrose inverse, we should use pinv() function in matlab instead of inv()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "    -(x^{(1)})^{T}-\\\\\n",
    "    -(x^{(2)})^{T}-\\\\\n",
    "    \\vdots    \\\\\n",
    "    -(x^{(m)})^{T}-\n",
    "\\end{bmatrix}\n",
    "$  &ensp;&ensp;&ensp;&ensp;$\\vec{y}^{\\,} = \n",
    "\\begin{bmatrix}\n",
    "    y^{(1)}\\\\\n",
    "    y^{(2)}\\\\\n",
    "    \\vdots    \\\\\n",
    "    y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore $X\\theta - \\vec{y}^{\\,} = \n",
    "\\begin{bmatrix}\n",
    "    h_{\\theta}x^{(1)} - y^{(1)}\\\\\n",
    "    h_{\\theta}x^{(2)} - y^{(2)}\\\\\n",
    "    \\vdots    \\\\\n",
    "    h_{\\theta}x^{(m)} - y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vector $z$ we have $z^{T}z = \\sum_{i}z_{i}^{2}$    \n",
    "### Therefore we have $\\frac{1}{2}(X\\theta - \\vec{y}^{\\,})^{T}(X\\theta - \\vec{y}^{\\,}) = \\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^{2} = J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/normalEquationProof.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus to minimize J we set derivatives to zero,    \n",
    "## $X^{T}X\\theta = X^{T}\\vec{y}^{\\,}$    \n",
    "## $\\theta = (X^{T}X)^{-1}X^{T}\\vec{y}^{\\,}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gradientDescentVsNormalEquation.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is viable to use normal equation if $n < 10^{6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are interested in binary classification i.e $y\\in\\{0, 1\\}$ , thus now we use the bernoulli distribution\n",
    "and our hypotheses function\n",
    "\n",
    "## $h_\\theta(x) = \\frac{1}{1+e^-{\\theta^{T}x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}cost(h_{\\theta}(x^{(i)}), y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cost(h_{\\theta}(x), y) = -log(h_\\theta(x))$&ensp;&ensp; if y = 1\n",
    "\n",
    "$cost(h_{\\theta}(x), y) = -log(1-h_\\theta(x))$&ensp;&ensp; if y = 0\n",
    "\n",
    "above equation equivalent to \n",
    "\n",
    "$cost = -(y)log(h_\\theta(x)) - (1-y)log(1-h_\\theta(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $J(\\theta) = \\frac{-1}{m}\\sum_{i=1}^{m}(y)log(h_\\theta(x)) + (1-y)log(1-h_\\theta(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying gradient descent we arrive with the same equation we had arrived at in linear regression i.e\n",
    "\n",
    "### Repeat until convergence :\n",
    "### $\\theta_{j} := \\theta_{j} - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)} $   (for every j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In vectorized form:\n",
    "$\\theta := \\theta - \\frac{\\alpha}{m} \\cdot X^{T}(g(X\\theta)-y))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read more about below methods\n",
    "<i>\"Conjugate gradient\", \"BFGS\", and \"L-BFGS\" are more sophisticated, faster ways to optimize θ that can be used instead of gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "in this case he have $y\\in\\{0, 1, 2\\dots n\\}$, thus we will consider $(n+1)$ binary classification problems and in each case we will consider $y = i$ as as one of our classes\n",
    "\n",
    "$h_{\\theta}^{(i)}(x) = P(y = i \\mid x;\\theta)$&ensp;&ensp;&ensp;$(i=0, 1\\dots n)$\n",
    "\n",
    "$prediction = max(h_{\\theta}^{(i)}(x))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting\n",
    "Underfiting or high bias occurs when number of parameters is low, thus affecting prediction.\\\n",
    "Overfitting occurs when we have too many parameters, thus working perfectly on training set but poor prediction for unseen data.\\\n",
    "2 ways to solve overfitting:\n",
    "#### Reduce number of features.\n",
    "    1.Manually select features.\n",
    "    2.Use model selection algorithm.\n",
    "#### Regularization.\n",
    "    1.Keep all features but reduce magnitude of parameters.\n",
    "    2.Regularization works well when we have a lot of useful features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to make the function $\\theta_{0}+\\theta_{1}x+\\theta_{2}x^{2}+\\theta_{3}x^{3}+\\theta_{4}x^{4}$ more quadratic we will try to reduce the influence of $\\theta_{3}x^{3}+\\theta_{4}x^{4}$\\\n",
    "thus we will add the following terms: $1000*\\theta_{3}^{2} + 1000*\\theta_{4}^{2}$\\\n",
    "this will inflate the cost of $\\theta_{3}$ and $\\theta_{4}$ and thus they would have to be $\\approx 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we could regularize all our parameters in the following way,\\\n",
    "$J(\\theta) = \\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2} + \\lambda\\sum_{j=1}^{n}\\theta_{j}^{2}$\\\n",
    "here $\\lambda$ is called the regularization parameter which decides how much to inflate the cost of parameters.\\\n",
    "choosing large lambda may cause underfitting and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "$J(\\theta) = \\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2} + \\lambda\\sum_{j=1}^{n}\\theta_{j}^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "we will seperate $\\theta_{0}$ because we dont want to penalize $\\theta_{0}$\n",
    "\n",
    "Repeat {\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta_{0} := \\theta_{0} - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)}$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta_{j} := \\theta_{j} - \\alpha[\\frac{1}{m} \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_{j}]$&ensp;&ensp;&ensp;&ensp;$j\\in\\{1, 2\\dots n\\}$\n",
    "\n",
    "}\n",
    "\n",
    "This term performs normalization: $\\frac{\\lambda}{m}\\theta_{j}$\n",
    "\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha[\\frac{1}{m} \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_{j}]$\n",
    "equivalent to \n",
    "\n",
    "$\\theta_{j} := \\theta_{j}(1-\\alpha\\cdot\\frac{\\lambda}{m}) - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)}$\n",
    "\n",
    "In normalized form:\n",
    "$\\theta = (X^{T}X + \\lambda L)^{-1}X^{T}y$\n",
    "\n",
    "where L = \n",
    "$\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "&1\\\\\n",
    "&&1\\\\\n",
    "&&&\\ddots\\\\\n",
    "&&&&1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "dimensions of L are n+1 X n+1.\n",
    "If m < n $X^{T}X$ is non-invertible but after adding $\\lambda$ term it becomes invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "$J(\\theta) = \\frac{-1}{m}\\sum_{i=1}^{m}[(y)log(h_{\\theta}(x)) + (1-y)log(1-h_{\\theta}(x))] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "Repeat {\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta_{0} := \\theta_{0} - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)}$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta_{j} := \\theta_{j} - \\alpha[\\frac{1}{m} \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_{j}]$&ensp;&ensp;&ensp;&ensp;$j\\in\\{1, 2\\dots n\\}$\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
