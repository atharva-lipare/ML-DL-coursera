{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-229 Personal Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Hypotheses.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^{(i)}$ denotes I/P variables of $i'th$ training example.   \n",
    "$y^{(i)}$ denotes corresponding O/P  \n",
    "A pair $(x^{(i)}, y^{(i)})$ called a training example.  \n",
    "Set of $m$ training examples called training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here h is the function dependent on various features / parameters.\n",
    "Eg:-\n",
    "    Let cost of house be $y $, area be $x1$ and no. of room $x2$.  \n",
    "    $ h_{\\theta} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{1}x_{2} $  \n",
    "    $ \\theta_{i}$'s are called the parameters / weights.  \n",
    " ### $ h_{\\theta} = \\sum_{i=0}^{m}\\theta_{i}x_{i} = \\theta^{T}x$, where $x_{0} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta)$ denoted as the cost function.  \n",
    "### $J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMS Algorithm (Least mean square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose $\\theta$ such that to minimize $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with some initial guess for $\\theta$ and then repeatedly change $\\theta$ to minimize $J(\\theta)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat until convergence :\n",
    "### $\\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above rule called LMS update rule and also Widrow-Hoff learning rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here $\\alpha$ is called the learning rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we **simulataneosly update for all values of $j$.**  \n",
    "i.e  \n",
    "    $temp0 := \\theta_{0} - \\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}$  \n",
    "    $temp1 := \\theta_{1} - \\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_{1}}$  \n",
    "    $\\theta_{0} := temp0$  \n",
    "    $\\theta_{1} := temp1$      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} = \\frac{\\partial\\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^{2}}{\\partial \\theta_{j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  $ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}= \\frac{1}{2} \\cdot 2 \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot \\frac{\\partial (h_{\\theta}x^{(i)})}{\\partial \\theta_{j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}= \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat until convergence :\n",
    "### $\\theta_{j} := \\theta_{j} - \\alpha \\cdot \\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)} $   (for every j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This method looks at every traing example in the entire training set on every step, thus called batch gradient descent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/batchGradientDescent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above example we start at the above star point and then descend in the direction of steepest slope and then reach local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch gradient descent algo runs slow when training set is large, alternative algo below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent (incremental gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop {  \n",
    "&ensp;&ensp;&ensp;&ensp;for i = 1 to m {  \n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;$\\theta_{j} := \\theta_{j} - \\alpha \\cdot (h_{\\theta}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)}$ (for every $j$)            \n",
    "&ensp;&ensp;&ensp;&ensp;}  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic is faster than batch algo, here parameters are updated after visiting a training example.\n",
    "However stichastic may never converge to a minimum, but will oscillate around the minimum .\n",
    "Stochastic preferred when training set is large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent feature scaling\n",
    "\n",
    "Gradient descent can be sped up by bringing each of input values in roughly the same range.\n",
    "This is becoz $/theta$ descends faster on small ranges and slower on larger ranges.\n",
    "For this we use feature scaling and mean normalization. Feature scaling corresponds to divind input by range i.e $(max - min)$. Mean normalization is subtracting input by mean of input over set.\n",
    "e.g :- \n",
    "\n",
    "$x_{i}$ ranges from $10$ to $30$ and its mean over the range is $23$ then $x_{i} = \\frac{x_{i}-23}{20}$\n",
    "\n",
    "thus,\n",
    "\n",
    "$x_{i} = \\frac{x_{i}-\\mu}{s_{i}}$\n",
    "\n",
    "where $\\mu$ is the mean, $s_{i}$ can be the standard deviation or $max-min$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging gradient descent: \n",
    "plot $J(\\theta)$ against number of iterations, if $J(\\theta)$ increases then $\\alpha$ is too largec\n",
    "\n",
    "if $\\alpha$ is too small, slow convergence, if $\\alpha$ is too large, cost function may overshoot and not converge.\n",
    "Try from $\\alpha = 0.001$ and then experiment with $3 * \\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\theta$ can also be found using the following equation:    \n",
    "### $\\theta = (X^{T}X)^{-1}X^{T}Y$    \n",
    "where $X$ is the m-by-n+1 design matrix containing all training example input values in it's rows,  $\\vec{y}^{\\,}$ is the m-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "    -(x^{(1)})^{T}-\\\\\n",
    "    -(x^{(2)})^{T}-\\\\\n",
    "    \\vdots    \\\\\n",
    "    -(x^{(m)})^{T}-\n",
    "\\end{bmatrix}\n",
    "$  &ensp;&ensp;&ensp;&ensp;$\\vec{y}^{\\,} = \n",
    "\\begin{bmatrix}\n",
    "    y^{(1)}\\\\\n",
    "    y^{(2)}\\\\\n",
    "    \\vdots    \\\\\n",
    "    y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore $X\\theta - \\vec{y}^{\\,} = \n",
    "\\begin{bmatrix}\n",
    "    h_{\\theta}x^{(1)} - y^{(1)}\\\\\n",
    "    h_{\\theta}x^{(2)} - y^{(2)}\\\\\n",
    "    \\vdots    \\\\\n",
    "    h_{\\theta}x^{(m)} - y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vector $z$ we have $z^{T}z = \\sum_{i}z_{i}^{2}$    \n",
    "### Therefore we have $\\frac{1}{2}(X\\theta - \\vec{y}^{\\,})^{T}(X\\theta - \\vec{y}^{\\,}) = \\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^{2} = J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/normalEquationProof.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus to minimize J we set derivatives to zero,    \n",
    "## $X^{T}X\\theta = X^{T}\\vec{y}^{\\,}$    \n",
    "## $\\theta = (X^{T}X)^{-1}X^{T}\\vec{y}^{\\,}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gradientDescentVsNormalEquation.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is viable to use normal equation if $n < 10^{6}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
